package com.platform.main;

import com.platform.dashboard.AnalyticsDashboard;
import com.platform.ingestion.DataIngestionEngine;
import com.platform.model.DataRecord;
import com.platform.model.Anomaly;
import com.platform.model.Metric;
import com.platform.monitor.PerformanceMonitor;
import com.platform.processing.StreamProcessingEngine;

import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.time.Duration;
import java.util.List;
import java.util.Random;
import java.util.Map;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.function.Function;
import java.util.stream.Collectors;
import java.util.stream.Stream;

public class AnalyticsPlatformMain {

    /**
     * Ph∆∞∆°ng th·ª©c kh·ªüi ƒë·ªông n·ªÅn t·∫£ng x·ª≠ l√Ω Stream Analytics.
     */
    public static void main(String[] args) throws IOException, InterruptedException {

        System.out.println("=================================================");
        System.out.println("üöÄ ANALYTICS PLATFORM: STARTING STREAM PROCESSING");
        System.out.println("=================================================");

        // Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn h·ªá th·ªëng
        DataIngestionEngine ingestion = new DataIngestionEngine();
        StreamProcessingEngine processing = new StreamProcessingEngine();
        AnalyticsDashboard dashboard = new AnalyticsDashboard();
        PerformanceMonitor monitor = new PerformanceMonitor(dashboard);

        // --- PHASE 1: BATCH PROCESSING (D·ªØ li·ªáu L·ªãch s·ª≠) ---
        // Pipeline n√†y s·∫Ω ch·∫°y ƒë·ªìng b·ªô v√† thu th·∫≠p k·∫øt qu·∫£ v√†o List
        List<DataRecord> historicalData = processHistoricalData(ingestion, processing, monitor);

        // --- PHASE 2: REAL-TIME STREAM PROCESSING ---
        // Pipeline n√†y s·∫Ω ch·∫°y kh√¥ng ƒë·ªìng b·ªô (b·∫±ng ExecutorService)
        ExecutorService rtExecutor = startRealTimeProcessing(ingestion, processing, monitor, dashboard);

        // Gi·ªØ lu·ªìng ch√≠nh ch·∫°y trong 10 gi√¢y ƒë·ªÉ quan s√°t Real-time metrics
        System.out.println("\n[MAIN THREAD] Ch·ªù 10 gi√¢y ƒë·ªÉ quan s√°t Real-time Metrics...");
        try {
            Thread.sleep(10000);
        } finally {
            rtExecutor.shutdownNow(); // ƒê·∫£m b·∫£o d·ª´ng lu·ªìng x·ª≠ l√Ω Real-time
        }

        // --- PHASE 3: ANALYTICS V√Ä REPORTING (Sau khi d·ª´ng Real-time) ---
        System.out.println("\n=================================================");
        System.out.println("üìä FINAL REPORTING");
        System.out.println("=================================================");

        // 1. Interactive Queries (D√πng d·ªØ li·ªáu l·ªãch s·ª≠)
        dashboard.executeQuery("Find Top 5 Users by Average Value", historicalData);

        // 2. Performance Analysis
        System.out.println(monitor.analyzePerformance());

        System.out.println("‚úÖ PLATFORM DEMO COMPLETED.");
    }

    private static List<DataRecord> processHistoricalData(
            DataIngestionEngine ingestion,
            StreamProcessingEngine processing,
            PerformanceMonitor monitor) throws IOException {

        Path mockFilePath = Path.of("historical_data.csv");
        createMockCSVFile(mockFilePath, 5000);

        System.out.println("--- 1. BATCH PROCESSING (Historical) ---");

        Stream<DataRecord> batchStream = ingestion.readCSV(mockFilePath);

        Stream<DataRecord> processedBatchStream =
                pipe(batchStream, s -> monitor.monitorThroughput(s, "Batch_E"));

        processedBatchStream =
                pipe(processedBatchStream, processing::clean);

        processedBatchStream =
                pipe(processedBatchStream, processing::enrich);

        processedBatchStream =
                pipe(processedBatchStream, s -> processing.detectAnomalies(s).map(a -> {
                    System.out.println("ANOMALY DETECTED: " + a.toString());

                    return new DataRecord(
                            "Anomaly_Detector",
                            a.getKey(),
                            a.getCurrentValue(),
                            Map.of("status", "ANOMALY", "reason", a.getReason())
                    );
                }));

        processedBatchStream =
                pipe(processedBatchStream, s -> monitor.monitorLatency(s, "Batch_T"));

        // Gi·∫£ l·∫≠p Load: Collect th√†nh List
        return processedBatchStream
                .peek(System.out::println)
                .collect(Collectors.toList());
    }

    private static ExecutorService startRealTimeProcessing(
            DataIngestionEngine ingestion,
            StreamProcessingEngine processing,
            PerformanceMonitor monitor,
            AnalyticsDashboard dashboard) {

        System.out.println("\n--- 2. REAL-TIME PROCESSING (Kafka Mock) ---");

        Stream<DataRecord> realTimeSource = ingestion.readFromKafka("events_topic");

        ExecutorService executor = Executors.newSingleThreadExecutor();
        executor.submit(() -> {

            // REAL-TIME Pipeline
            Stream<DataRecord> rtStream = pipe(realTimeSource, s -> monitor.monitorLatency(s, "RealTime_E"));
            rtStream = pipe(rtStream, processing::clean);

            Stream<Metric> aggregatedStream = processing.aggregate(rtStream, Duration.ofSeconds(5));

            // LOAD: G·ª≠i Metrics ƒë·∫øn Dashboard (S·ª≠ d·ª•ng forEach v√† b·ªçc Metric)
            aggregatedStream.forEach(metric -> {
                dashboard.updateMetrics(Stream.of(metric));
            });
        });

        return executor; // Tr·∫£ v·ªÅ Executor ƒë·ªÉ c√≥ th·ªÉ qu·∫£n l√Ω (shutdown)
    }

    // Helper ƒë·ªÉ ƒë∆°n gi·∫£n h√≥a vi·ªác n·ªëi chu·ªói c√°c Stream methods
    private static <T> Stream<T> pipe(Stream<T> stream, Function<Stream<T>, Stream<T>> transformation) {
        return transformation.apply(stream);
    }

    private static void createMockCSVFile(Path path, int lines) throws IOException {
        try (var writer = Files.newBufferedWriter(path)) {
            writer.write("key,value,location\n");
            Random rnd = new Random();
            for (int i = 0; i < lines; i++) {
                String key = "User_" + (i % 100);
                double value = 15.0 + rnd.nextGaussian() * 8.0 + (i % 100 == 0 ? 50.0 : 0.0);
                String location = "Zone_" + (i % 5);
                writer.write(String.format("%s,%.2f,%s\n", key, value, location));
            }
        }
    }
}